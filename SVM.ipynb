{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Classification: Decision\n",
        "Trees, SVM, and Naive Bayes|\n",
        "Assignment\n"
      ],
      "metadata": {
        "id": "ClevP05WANQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1 :  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "  Information Gain (IG) measures how well a feature splits the data. It is calculated as the reduction in entropy after a dataset is split on an attribute. In Decision Trees, IG helps select the best feature at each node by choosing the one that maximizes the reduction in uncertainty (entropy).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PLOXxwjgASQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Gini Impurity measures the chance of incorrect classification:\n",
        "Gini = 1 - \\sum p_i^2\n",
        "Entropy measures the uncertainty in data:\n",
        "Entropy = -\\sum p_i \\log_2 p_i\n",
        "\n",
        " Gini is faster and often used in CART.\n",
        "\n",
        " Entropy is more sensitive to class distribution and used in ID3/C4.5.\n"
      ],
      "metadata": {
        "id": "lbhJLTRuAmfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        " Pre-Pruning (or early stopping) halts tree growth before it becomes overly complex. It sets constraints like maximum depth, minimum samples per leaf, or minimum information gain. This prevents overfitting and improves generalization.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ce6V2uvoBH5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances"
      ],
      "metadata": {
        "id": "b_j-C5MOBPW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "\n",
        "for name, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivwA_jmaBYC6",
        "outputId": "5c1f678e-8faa-4ab5-9cf1-02b6c70280c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0133\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        " SVM is a supervised learning algorithm that finds the optimal hyperplane to separate classes. It maximizes the margin between support vectors (critical data points) and the decision boundary, improving generalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8D4B_Nn-Bo5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6:  What is the Kernel Trick in SVM?\n",
        "\n",
        " The Kernel Trick allows SVMs to operate in high-dimensional spaces without explicitly computing coordinates. It uses kernel functions (like RBF or polynomial) to compute dot products in transformed spaces, enabling non-linear classification.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V79hbe5KB2xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7:  Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "cfUYC-4ZCCm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {acc_rbf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNfj4cpECGxX",
        "outputId": "2a4d5658-463b-4c25-d132-d71f4f3a39a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        " Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes feature independence, which is rarely true in practice—hence the term \"naïve.\" Despite this, it performs well in many tasks like spam filtering and text classification.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wezVxhePCXoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "1.Gaussian Naïve Bayes\n",
        "\n",
        "Used for: Continuous (numeric) data\n",
        "\n",
        "Assumes: Features follow a normal (bell curve) distribution\n",
        "\n",
        "Example: Height, weight, temperature, or income data\n",
        "\n",
        "Use case: Medical data classification, sensor readings\n",
        "\n",
        "2.Multinomial Naïve Bayes\n",
        "\n",
        "Used for: Discrete or count-based data\n",
        "\n",
        "Assumes: Features represent frequency or counts (like how many times a word appears)\n",
        "\n",
        "Example: Word counts in emails or documents\n",
        "\n",
        "Use case: Spam detection, text classification\n",
        "\n",
        "3.Bernoulli Naïve Bayes\n",
        "\n",
        "Used for: Binary (yes/no or 0/1) data\n",
        "\n",
        "Assumes: Each feature is either present or absent\n",
        "\n",
        "Example: Whether a specific word appears in a message (yes or no)\n",
        "\n",
        "Use case: Sentiment analysis, short text classification\n"
      ],
      "metadata": {
        "id": "39ataX21Cgm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10:  Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "h6t0hdLpDmsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\" Gaussian Naïve Bayes Accuracy on Breast Cancer dataset:\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-h_mnbwD0ui",
        "outputId": "3e5d0600-539d-4f3c-af20-801f2e27a163"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gaussian Naïve Bayes Accuracy on Breast Cancer dataset: 97.37 %\n"
          ]
        }
      ]
    }
  ]
}